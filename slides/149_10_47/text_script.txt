and let's just think about the basic sequence of a conv layer
we might do this big matrix multiplication and output a width by height by n by k output tensor

we're going to store that in memory

and then we're going to bring it right back in and do something like add in the bias
and then we're going to store that in memory

and we're going to bring it right back in and do a max pull

so this tensor is going in and out of memory over and over and over again

and for these
conv layer might be blockable
but scale and bias is not

the only thing you're doing is multiplying every element by a number and max pool
there's not a lot of math in there i'm just taking every 2 by 2 region of the tensor

computing the max and outputting that so these things here are severely bandwidth bound