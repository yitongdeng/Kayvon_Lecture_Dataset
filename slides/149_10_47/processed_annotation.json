[{"bboxes": [[452, 274, 1688, 456]], "text": "and let's just think about the basic sequence of a conv layer", "start": 0.0, "end": 4.420000076293945}, {"bboxes": [[590, 312, 829, 456]], "text": "we might do this big matrix multiplication", "start": 4.699999809265137, "end": 6.800000190734863}, {"bboxes": [[810, 272, 944, 296]], "text": "and output a width by height by n by k output tensor", "start": 6.800000190734863, "end": 12.9399995803833}, {"bboxes": [[]], "text": "we're going to store that in memory, and then we're going to bring it right back in", "start": 13.5, "end": 20.860000610351562}, {"bboxes": [[928, 313, 1167, 454]], "text": "and do something like add in the bias", "start": 20.860000610351562, "end": 23.540000915527344}, {"bboxes": [[]], "text": "and then we're going to store that in memory, and we're going to bring it right back in", "start": 23.760000228881836, "end": 29.219999313354492}, {"bboxes": [[1262, 313, 1501, 454]], "text": "and do a max pool", "start": 29.219999313354492, "end": 30.440000534057617}, {"bboxes": [[]], "text": "so this tensor is going in and out of memory over and over and over again", "start": 31.360000610351562, "end": 35.619998931884766}, {"bboxes": [[590, 312, 829, 456]], "text": "and for these, conv layer might be blockable", "start": 37.81999969482422, "end": 40.900001525878906}, {"bboxes": [[928, 313, 1167, 454]], "text": "but scale and bias is not, the only thing you're doing is multiplying every element by a number", "start": 41.29999923706055, "end": 45.97999954223633}, {"bboxes": [[1262, 313, 1501, 454]], "text": "and max pool, there's not a lot of math in there i'm just taking every 2 by 2 region of the tensor, computing the max and outputting that", "start": 46.060001373291016, "end": 53.720001220703125}, {"bboxes": [[928, 313, 1167, 454], [1262, 313, 1501, 454]], "text": "so these things here are severely bandwidth bound", "start": 53.720001220703125, "end": 57.380001068115234}]