[{"bboxes": [[495, 458, 577, 631]], "text": "so in hadoop, this is the case, they're doing logistic regression, which is a very simple ml operation", "start": 0.7799999713897705, "end": 8.220000267028809}, {"bboxes": [[]], "text": "and k means, which you're very familiar with because of course you've played with that", "start": 9.119999885559082, "end": 13.039999961853027}, {"bboxes": [[496, 458, 532, 591]], "text": "so we see that hadoop, the first iteration is 80 seconds", "start": 13.779999732971191, "end": 21.280000686645508}, {"bboxes": [[537, 461, 572, 591]], "text": "and subsequent iterations don't get much faster", "start": 21.34000015258789, "end": 23.739999771118164}, {"bboxes": [[]], "text": "of course each iteration requires an hdfs read and an hdfs write", "start": 24.299999237060547, "end": 29.639999389648438}, {"bboxes": [[]], "text": "the hadoop binary memory basically keeps a binary memory copy instead of a text copy", "start": 34.400001525878906, "end": 42.060001373291016}, {"bboxes": [[495, 458, 577, 631]], "text": "and so it's slightly faster on the second iteration", "start": 42.36000061035156, "end": 44.65999984741211}, {"bboxes": [[]], "text": "but still has to access the disk", "start": 44.97999954223633, "end": 46.7599983215332}, {"bboxes": [[733, 495, 808, 633]], "text": "then spark only has to do the hdfs read, doesn't do the write just write straight to memory, and so subsequent iterations are much faster", "start": 47.720001220703125, "end": 56.84000015258789}]